{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Divvy Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook should only be run once. No need to run it again once we have `data/processed/divvy_dataset.csv`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run all cells in this notebook to obtain filtered divvy data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_RAW = \"data/raw/\"\n",
    "PATH_PROCESSED = \"data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, we are interested in Divvy data from the month of **October 2022**. This data is available from the [Divvy data index](https://divvy-tripdata.s3.amazonaws.com/index.html) and can be downloaded as a `.zip` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"202210-divvy-tripdata.zip\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first run commands to set up the folders that will store the datasets. These particular commands work for MacOS devices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: data: File exists\n"
     ]
    }
   ],
   "source": [
    "! mkdir data; cd data; mkdir raw;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we try to download the file from the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "\n",
    "def download_zip(url, output_path):\n",
    "  \"\"\"Download the file from the url given into the specified folder\n",
    "  \"\"\"\n",
    "  try:\n",
    "    response = requests.get(url)\n",
    "    filename = url.split('/')[-1]\n",
    "    with open(os.path.join(output_path, filename), 'wb') as output_file:\n",
    "      output_file.write(response.content)\n",
    "    print(\"Download complete:\", filename)\n",
    "    response.close()\n",
    "  except requests.ConnectionError as error:\n",
    "    print(\"A problem occured during download.\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://divvy-tripdata.s3.amazonaws.com/\" + filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download complete: 202210-divvy-tripdata.zip\n"
     ]
    }
   ],
   "source": [
    "download_zip(url, PATH_RAW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we should have the raw data zip file. We can extract the zip file and delete it after."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zipfile import ZipFile\n",
    "\n",
    "with ZipFile(PATH_RAW + filename, 'r') as zip_obj:\n",
    "  zip_obj.extractall(PATH_RAW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "! cd data/raw/; rm *.zip;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for the fun part, filtering the data with pandas. We only want to remove the `NaN` values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"202210-divvy-tripdata.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(PATH_RAW + filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And thus, we have our dataset with removed `NaN` values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(414269, 13)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is actually still a little bit too large, as this will create a ~100MB `.csv` file, when we need one that is <50MB for Observable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we just want to pick data from **one week** in September. Specifically, rides occuring between **Monday, 24th October 2022** and **Sunday, 30th October 2022** (inclusive). We also wish to add two extra attributes that can be derived from the raw dataset, which are _distance_ and _duration_ of the trips. This will help us answer our questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select only one week of data, due to size constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (df[\"started_at\"] > \"2022-10-24 00:00:01\") & (df[\"started_at\"] <= \"2022-10-30 23:59:59\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "subsetdf = df.loc[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(84747, 13)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subsetdf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shrinks our dataset size to about 20% from the normal dataset. We believe meaningful knowledge can still be obtained even with this constraint."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare to export dataframe to file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "subsetdf = subsetdf.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "subsetdf.to_csv(PATH_PROCESSED + \"divvy_dataset.csv\", index=False)  # index=False important for Observable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, **manually** delete the `data/raw/` folder."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('geo')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bacff2f7efbfdd8d71a6c50169b3d83c99ff5a2c029bff82fef683bfe05a058e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
